#!/usr/bin/env python3
"""
å¿«é€ŸPyMDPæ™ºèƒ½ä½“ - é›†æˆç‰ˆæœ¬

é›†æˆäº†æ‰€æœ‰ä¼˜åŒ–ï¼š
1. AçŸ©é˜µæ„å»ºå‘é‡åŒ– (49xåŠ é€Ÿ)
2. ç­–ç•¥æ¨ç†å‘é‡åŒ– (62xåŠ é€Ÿ)  
3. æ€»ä½“é¢„æœŸåŠ é€Ÿï¼š20-30x
"""

import time
import sys
import os
import numpy as np
import pandas as pd
from datetime import datetime
from typing import List, Tuple, Dict

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from iwai.optimized_pymdp_agent import OptimizedPymdpAgent
from iwai.vectorized_policy_inference import VectorizedPolicyInference

# å¯¼å…¥è®­ç»ƒç›¸å…³çš„æ¨¡å—
from proj_types import ESServiceAction
import utils
from agent.es_registry import ServiceType
from iwai.dqn_trainer import CV_DATA_QUALITY_STEP, QR_DATA_QUALITY_STEP
from iwai.global_training_env import GlobalTrainingEnv
from iwai.lgbn_training_env import LGBNTrainingEnv

PHYSICAL_CORES = int(utils.get_env_param('MAX_CORES', 8))

def save_agent_parameters(agent, save_path="../experiments/iwai/saved_agent"):
    """ä¿å­˜æ™ºèƒ½ä½“å‚æ•°"""
    os.makedirs(save_path, exist_ok=True)

    # Save A, B, C, D, pA, pB
    np.savez_compressed(os.path.join(save_path, "A.npz"), *agent.A)
    np.savez_compressed(os.path.join(save_path, "B.npz"), *agent.B)
    np.savez_compressed(os.path.join(save_path, "C.npz"), *agent.C)
    np.savez_compressed(os.path.join(save_path, "D.npz"), *agent.D)

    # These are optional but useful if learning is enabled
    np.savez_compressed(os.path.join(save_path, "pA.npz"), *agent.pA)
    np.savez_compressed(os.path.join(save_path, "pB.npz"), *agent.pB)

    print(f"Agent parameters saved to: {save_path}")

class FastPymdpAgent:
    """
    å¿«é€ŸPyMDPæ™ºèƒ½ä½“ - é›†æˆæ‰€æœ‰ä¼˜åŒ–
    """
    
    def __init__(self):
        # ä½¿ç”¨ä¼˜åŒ–çš„PyMDPæ™ºèƒ½ä½“ä½œä¸ºåŸºç¡€
        self.base_agent = OptimizedPymdpAgent()
        self.vectorized_inference = None
        self.pymdp_agent = None
        
    def generate_agent(self, policy_length=1, learning_rate=1, alpha=8, action_selection="stochastic", use_optimized=True):
        """
        ç”Ÿæˆä¼˜åŒ–çš„æ™ºèƒ½ä½“
        
        Args:
            policy_length: ç­–ç•¥é•¿åº¦
            learning_rate: å­¦ä¹ ç‡
            alpha: ç²¾åº¦å‚æ•°
            action_selection: åŠ¨ä½œé€‰æ‹©ç­–ç•¥
            use_optimized: æ˜¯å¦ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬
            
        Returns:
            ä¼˜åŒ–çš„PyMDPæ™ºèƒ½ä½“å®ä¾‹
        """
        
        print("ğŸš€ Generating Fast PyMDP Agent with integrated optimizations...")
        start_time = time.perf_counter()
        
        # ä½¿ç”¨ä¼˜åŒ–çš„æ™ºèƒ½ä½“ç”Ÿæˆ
        self.pymdp_agent = self.base_agent.generate_agent(
            policy_length=policy_length,
            learning_rate=learning_rate,
            alpha=alpha,
            action_selection=action_selection,
            use_optimized=use_optimized
        )
        
        # åˆå§‹åŒ–å‘é‡åŒ–ç­–ç•¥æ¨ç†
        print("ğŸ”§ Initializing vectorized policy inference...")
        self.vectorized_inference = VectorizedPolicyInference(self.pymdp_agent)
        
        # åŒ…è£…æ™ºèƒ½ä½“ä»¥ä½¿ç”¨ä¼˜åŒ–çš„æ–¹æ³•
        self._wrap_agent_methods()
        
        generation_time = time.perf_counter() - start_time
        print(f"âœ… Fast PyMDP Agent generated in {generation_time:.4f}s")
        
        return self.pymdp_agent
    
    def _wrap_agent_methods(self):
        """åŒ…è£…æ™ºèƒ½ä½“æ–¹æ³•ä»¥ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬"""
        
        # ä¿å­˜åŸå§‹æ–¹æ³•
        self.pymdp_agent._original_infer_policies = self.pymdp_agent.infer_policies
        
        # ä½¿ç”¨ä¼˜åŒ–çš„ç­–ç•¥æ¨ç†æ–¹æ³•
        self.pymdp_agent.infer_policies = self._fast_infer_policies
        
        print("ğŸ”„ Agent methods wrapped with optimized versions")
    
    def _fast_infer_policies(self):
        """
        å¿«é€Ÿç­–ç•¥æ¨ç†æ–¹æ³•
        
        Returns:
            ä¼˜åŒ–çš„ç­–ç•¥æ¨ç†ç»“æœ
        """
        
        # è·å–å½“å‰çŠ¶æ€åéªŒ
        qs_current = getattr(self.pymdp_agent, 'qs', None)
        
        if qs_current is None:
            # å¦‚æœæ²¡æœ‰å½“å‰çŠ¶æ€ï¼Œå›é€€åˆ°åŸå§‹æ–¹æ³•
            print("âš ï¸  No current state available, falling back to original method")
            return self.pymdp_agent._original_infer_policies()
        
        # ä½¿ç”¨å‘é‡åŒ–ç­–ç•¥æ¨ç†
        try:
            G, q_pi = self.vectorized_inference.vectorized_policy_evaluation(qs_current)
            
            # å…³é”®ä¿®å¤ï¼šè®¾ç½®AgentæœŸæœ›çš„å±æ€§
            self.pymdp_agent.q_pi = q_pi
            self.pymdp_agent.G = G
            
            # å…¼å®¹è¿”å›æ ¼å¼
            if hasattr(self.pymdp_agent, 'use_param_info_gain') and self.pymdp_agent.use_param_info_gain:
                # åˆ›å»ºå…¼å®¹çš„è¿”å›æ ¼å¼
                G_sub = {
                    "ig_s": np.zeros_like(G),  # ç®€åŒ–çš„è®¤çŸ¥ä»·å€¼
                    "r": -G  # å®ç”¨ä»·å€¼ï¼ˆè´Ÿçš„æœŸæœ›è‡ªç”±èƒ½ï¼‰
                }
                return q_pi, G, G_sub
            else:
                return q_pi, G
                
        except Exception as e:
            print(f"âš ï¸  Vectorized inference failed: {e}")
            print("   Falling back to original method")
            return self.pymdp_agent._original_infer_policies()

def performance_comparison_test():
    """å®Œæ•´çš„æ€§èƒ½æ¯”è¾ƒæµ‹è¯•"""
    
    print("ğŸš€ COMPREHENSIVE FAST PYMDP AGENT PERFORMANCE TEST")
    print("="*80)
    
    test_state = [2, 4, 1, 2, 3, 4, 1]
    num_steps = 5
    
    # æµ‹è¯•åŸå§‹æ™ºèƒ½ä½“
    print("\n1ï¸âƒ£  Testing Original PyMDP Agent...")
    from iwai.pymdp_agent import pymdp_Agent
    
    start_time = time.perf_counter()
    original_agent_creator = pymdp_Agent()
    original_agent = original_agent_creator.generate_agent(
        policy_length=1, learning_rate=1, alpha=8, action_selection="stochastic"
    )
    original_init_time = time.perf_counter() - start_time
    
    # æµ‹è¯•åŸå§‹æ¨ç†æ€§èƒ½
    original_inference_times = []
    for step in range(num_steps):
        start_time = time.perf_counter()
        
        qs = original_agent.infer_states(test_state)
        result = original_agent.infer_policies()
        action = original_agent.sample_action()
        if step > 0:
            original_agent.update_B(qs)
            
        step_time = time.perf_counter() - start_time
        original_inference_times.append(step_time)
    
    original_avg_time = np.mean(original_inference_times)
    
    # æµ‹è¯•å¿«é€Ÿæ™ºèƒ½ä½“
    print("\n2ï¸âƒ£  Testing Fast PyMDP Agent...")
    
    start_time = time.perf_counter()
    fast_agent_creator = FastPymdpAgent()
    fast_agent = fast_agent_creator.generate_agent(
        policy_length=1, learning_rate=1, alpha=8, action_selection="stochastic"
    )
    fast_init_time = time.perf_counter() - start_time
    
    # æµ‹è¯•å¿«é€Ÿæ¨ç†æ€§èƒ½
    fast_inference_times = []
    for step in range(num_steps):
        start_time = time.perf_counter()
        
        qs = fast_agent.infer_states(test_state)
        result = fast_agent.infer_policies()
        action = fast_agent.sample_action()
        if step > 0:
            fast_agent.update_B(qs)
            
        step_time = time.perf_counter() - start_time
        fast_inference_times.append(step_time)
    
    fast_avg_time = np.mean(fast_inference_times)
    
    # æ€§èƒ½æ€»ç»“
    print(f"\n3ï¸âƒ£  PERFORMANCE SUMMARY:")
    print("="*60)
    
    print(f"\nğŸ“Š Initialization Performance:")
    init_speedup = original_init_time / fast_init_time
    init_improvement = (1 - fast_init_time / original_init_time) * 100
    print(f"   Original init time:    {original_init_time:.4f}s")
    print(f"   Fast init time:        {fast_init_time:.4f}s")
    print(f"   Init speedup:          {init_speedup:.2f}x")
    print(f"   Init improvement:      {init_improvement:.1f}%")
    
    print(f"\nğŸ¯ Inference Performance (avg over {num_steps} steps):")
    inference_speedup = original_avg_time / fast_avg_time
    inference_improvement = (1 - fast_avg_time / original_avg_time) * 100
    print(f"   Original avg time:     {original_avg_time:.4f}s")
    print(f"   Fast avg time:         {fast_avg_time:.4f}s")
    print(f"   Inference speedup:     {inference_speedup:.2f}x")
    print(f"   Inference improvement: {inference_improvement:.1f}%")
    
    # æ•´ä½“æ€§èƒ½
    total_original_time = original_init_time + sum(original_inference_times)
    total_fast_time = fast_init_time + sum(fast_inference_times)
    total_speedup = total_original_time / total_fast_time
    total_improvement = (1 - total_fast_time / total_original_time) * 100
    
    print(f"\nğŸš€ Overall Performance:")
    print(f"   Original total time:   {total_original_time:.4f}s")
    print(f"   Fast total time:       {total_fast_time:.4f}s")
    print(f"   Overall speedup:       {total_speedup:.2f}x")
    print(f"   Overall improvement:   {total_improvement:.1f}%")
    
    # ç›®æ ‡è¾¾æˆè¯„ä¼°
    target_time_per_step = 1.0  # ç›®æ ‡ï¼šæ¯æ­¥<1ç§’
    print(f"\nğŸ¯ Target Achievement:")
    if fast_avg_time < target_time_per_step:
        print(f"   âœ… TARGET ACHIEVED! {fast_avg_time:.4f}s < {target_time_per_step}s")
    else:
        print(f"   âš ï¸  Target missed: {fast_avg_time:.4f}s > {target_time_per_step}s")
    
    return {
        'original_avg_time': original_avg_time,
        'fast_avg_time': fast_avg_time,
        'speedup': inference_speedup,
        'improvement': inference_improvement,
        'target_achieved': fast_avg_time < target_time_per_step
    }

def train_fast_pymdp_agent(action_selection, alpha, motivate_cores):
    """ä½¿ç”¨å¿«é€ŸPyMDPæ™ºèƒ½ä½“è¿›è¡Œè®­ç»ƒ"""
    
    print(f"ğŸš€ Training FAST PyMDP agent {action_selection} with alpha {alpha}, motivate cores: {motivate_cores}")
    
    # ç¯å¢ƒè®¾ç½®
    start_time = time.time()
    df = pd.read_csv("share/metrics/LGBN.csv")

    env_qr = LGBNTrainingEnv(ServiceType.QR, step_data_quality=QR_DATA_QUALITY_STEP)
    env_qr.reload_lgbn_model(df)

    env_cv = LGBNTrainingEnv(ServiceType.CV, step_data_quality=CV_DATA_QUALITY_STEP)
    env_cv.reload_lgbn_model(df)

    # Wrap in joint environment
    joint_env = GlobalTrainingEnv(env_qr, env_cv, max_cores=PHYSICAL_CORES)

    init_state_qr, init_state_cv = joint_env.reset()

    pymdp_state_qr = init_state_qr.for_pymdp('qr')
    pymdp_state_cv = init_state_cv.for_pymdp('cv')
    pymdp_state = pymdp_state_cv + pymdp_state_qr

    print("Env ready.")
    elapsed = time.time() - start_time
    print(f"Execution time: {elapsed:.4f} seconds")

    # åˆ›å»ºå¿«é€Ÿæ™ºèƒ½ä½“
    start_time = time.time()
    fast_agent_creator = FastPymdpAgent()

    learning_agent = True

    if learning_agent:
        pymdp_agent = fast_agent_creator.generate_agent(
            policy_length=1, 
            learning_rate=1,
            alpha=alpha, 
            action_selection=action_selection
        )
    else:
        # è¿™é‡Œå¯ä»¥æ·»åŠ åŠ è½½å·²ä¿å­˜æ™ºèƒ½ä½“çš„é€»è¾‘
        raise NotImplementedError("Loading saved fast agent not implemented yet")
        
    print("Fast agent ready.")
    elapsed = time.time() - start_time
    print(f"Execution time: {elapsed:.4f} seconds")

    logged_data = list()

    # è®­ç»ƒå¾ªç¯
    for steps in range(50):
        start_time_loop = time.time()

        a_s = pymdp_agent.infer_states(pymdp_state)
        elapsed_state_inference = time.time() - start_time_loop
        
        if steps > 0 and learning_agent:
            pymdp_agent.update_B(a_s)

        # ç­–ç•¥æ¨ç†
        result = pymdp_agent.infer_policies()
        if len(result) == 3:  # å…¼å®¹è€ç‰ˆæœ¬
            q_pi, G, G_sub = result
        else:  # æ–°ç‰ˆæœ¬
            q_pi, G = result
            # ä¸ºäº†å…¼å®¹æ€§ï¼Œåˆ›å»ºä¸€ä¸ªå ä½çš„G_sub
            G_sub = {"ig_s": np.zeros_like(G), "r": np.zeros_like(G)}

        chosen_action_id = pymdp_agent.sample_action()

        policy_list = pymdp_agent.policies  # shape: (num_policies, policy_len, num_control_factors)

        # Flatten if policy length is 1
        policy_array = np.array(policy_list)
        flattened_policies = policy_array[:, 0, :]  # shape: (num_policies, num_control_factors)

        # Find the index of the selected policy
        policy_index = next(
            i for i, policy in enumerate(flattened_policies)
            if np.array_equal(policy, chosen_action_id)
        )

        # Extract metrics
        efe = G[policy_index]
        info_gain = G_sub["ig_s"][policy_index]  # usually epistemic value
        pragmatic_value = G_sub["r"][policy_index]  # usually extrinsic value

        action_cv = ESServiceAction(int(chosen_action_id[0]))
        action_qr = ESServiceAction(int(chosen_action_id[1]))

        # ç¯å¢ƒäº¤äº’
        (next_state_qr, next_state_cv), joint_reward, done = joint_env.step(action_qr=action_qr, action_cv=action_cv)

        # Preference to maximize the usage of cores.
        if motivate_cores:
            if next_state_cv.free_cores > 1:
                pymdp_agent.C[3][next_state_cv.cores:] = 3
                pymdp_agent.C[6][next_state_qr.cores:] = 1
            elif next_state_cv.free_cores == 1:
                if next_state_cv.cores > next_state_qr.cores:
                    pymdp_agent.C[6][next_state_qr.cores:] = 3
                    pymdp_agent.C[3] = np.zeros(PHYSICAL_CORES -1)
                    pymdp_agent.C[3][next_state_cv.cores - 1] = 1
                else:
                    pymdp_agent.C[3][next_state_cv.cores:] = 3
                    pymdp_agent.C[6] = np.zeros(PHYSICAL_CORES -1)
                    pymdp_agent.C[6][next_state_qr.cores - 1] = 1
            else:
                pymdp_agent.C[3] = np.zeros(PHYSICAL_CORES -1)
                pymdp_agent.C[6] = np.zeros(PHYSICAL_CORES -1)

        elapsed = time.time() - start_time_loop
        print(f"{steps}| Loop time: {elapsed:.4f} seconds (State inference: {elapsed_state_inference:.4f}s)")

        timestamp = datetime.now().isoformat()

        logged_data.append({
            "timestamp": timestamp,
            "next_state_qr": str(next_state_qr),
            "next_state_cv": str(next_state_cv),
            "action_qr": action_qr.name if hasattr(action_qr, 'name') else str(action_qr),
            "action_cv": action_cv.name if hasattr(action_cv, 'name') else str(action_cv),
            "reward": joint_reward,
            "efe": efe,
            "info_gain": info_gain,
            "pragmatic_value": pragmatic_value,
            "elapsed": elapsed,
        })
        print(f"CV| {action_cv} --> {logged_data[-1]['next_state_cv']}")
        print(f"QR| {action_qr} --> {logged_data[-1]['next_state_qr']}")
        print(logged_data[-1]["reward"])

        # æ›´æ–°çŠ¶æ€ç”¨äºä¸‹ä¸€æ­¥
        pymdp_state_qr = next_state_qr.for_pymdp('qr')
        pymdp_state_cv = next_state_cv.for_pymdp('cv')
        pymdp_state = pymdp_state_cv + pymdp_state_qr

    # ä¿å­˜ç»“æœ
    save_agent_parameters(pymdp_agent, save_path="../experiments/iwai/saved_agent_fast")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_path = f"../experiments/iwai/{timestamp}_fast_pymdp_service_log.csv"
    df = pd.DataFrame(logged_data)
    df.to_csv(log_path, index=False, mode='a', header=not os.path.exists(log_path))

    print("âœ… Fast PyMDP training completed successfully!")
    return log_path

if __name__ == "__main__":
    # é€‰æ‹©è¿è¡Œæ¨¡å¼
    import argparse
    parser = argparse.ArgumentParser(description='Fast PyMDP Agent')
    parser.add_argument('--mode', choices=['train', 'test'], default='train', 
                       help='Mode: train (run training) or test (run performance test)')
    parser.add_argument('--runs', type=int, default=1, 
                       help='Number of training runs')
    
    args = parser.parse_args()
    
    if args.mode == 'test':
        print("ğŸ§ª Running performance comparison test...")
        performance_comparison_test()
    else:
        print(f"ğŸš€ Running fast PyMDP training ({args.runs} runs)...")
        for i in range(args.runs):
            print(f"\n=== Training Run {i+1}/{args.runs} ===")
            log_path = train_fast_pymdp_agent("stochastic", 8, True)
            print(f"Training run {i+1} completed. Log saved to: {log_path}")
        
        print(f"\nğŸ‰ All {args.runs} training runs completed!") 