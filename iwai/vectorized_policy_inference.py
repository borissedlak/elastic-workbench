#!/usr/bin/env python3
"""
å‘é‡åŒ–ç­–ç•¥æ¨ç†ä¼˜åŒ–

å°†åŸæœ¬ä¸²è¡Œçš„ç­–ç•¥è¯„ä¼°æ”¹ä¸ºå‘é‡åŒ–å¹¶è¡Œè®¡ç®—ï¼Œé¢„æœŸ3-8å€åŠ é€Ÿ
"""

import time
import sys
import os
import numpy as np
from typing import List, Tuple, Dict

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from iwai.pymdp_agent import pymdp_Agent

class VectorizedPolicyInference:
    """å‘é‡åŒ–ç­–ç•¥æ¨ç†ä¼˜åŒ–å™¨"""
    
    def __init__(self, agent):
        self.agent = agent
        self.policies = np.array(agent.policies)  # (num_policies, policy_len, num_control_factors)
        self.num_policies = len(self.policies)
        self.policy_len = self.policies.shape[1]
        self.num_factors = len(agent.B)
        
        # é¢„è®¡ç®—ç­–ç•¥ç›¸å…³çš„ç´¢å¼•å’Œæƒé‡
        self._precompute_policy_indices()
        
    def _precompute_policy_indices(self):
        """é¢„è®¡ç®—ç­–ç•¥ç´¢å¼•ä»¥åŠ é€Ÿåç»­è®¡ç®—"""
        
        print(f"ğŸ”§ Precomputing policy indices for {self.num_policies} policies...")
        
        # å°†ç­–ç•¥é‡å¡‘ä¸ºä¾¿äºå‘é‡åŒ–å¤„ç†çš„æ ¼å¼
        # policies: (num_policies, policy_len, num_control_factors)
        # å¯¹äºpolicy_len=1çš„æƒ…å†µï¼Œç®€åŒ–ä¸º (num_policies, num_control_factors)
        if self.policy_len == 1:
            self.flat_policies = self.policies[:, 0, :]  # (35, 2)
        else:
            self.flat_policies = self.policies
            
        print(f"  Flattened policies shape: {self.flat_policies.shape}")
        
        # ä¸ºæ¯ä¸ªçŠ¶æ€å› å­å‡†å¤‡ç­–ç•¥ç›¸å…³çš„åŠ¨ä½œç´¢å¼•
        self.policy_action_indices = {}
        for factor_idx in range(self.num_factors):
            # è·å–å½±å“è¯¥å› å­çš„æ§åˆ¶å› å­
            control_factors = self.agent.B_factor_control_list[factor_idx]
            
            if len(control_factors) == 1:
                # å•ä¸€æ§åˆ¶å› å­
                control_idx = control_factors[0]
                self.policy_action_indices[factor_idx] = self.flat_policies[:, control_idx]
            else:
                # å¤šä¸ªæ§åˆ¶å› å­
                self.policy_action_indices[factor_idx] = self.flat_policies[:, control_factors]
                
        print(f"  Precomputed action indices for {self.num_factors} factors")
        
    def vectorized_policy_evaluation(self, qs_current: List[np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:
        """
        å‘é‡åŒ–ç­–ç•¥è¯„ä¼°
        
        Args:
            qs_current: å½“å‰çŠ¶æ€åéªŒåˆ†å¸ƒåˆ—è¡¨
            
        Returns:
            G: æ‰€æœ‰ç­–ç•¥çš„æœŸæœ›è‡ªç”±èƒ½ (num_policies,)
            q_pi: ç­–ç•¥æ¦‚ç‡åˆ†å¸ƒ (num_policies,)
        """
        
        print(f"ğŸš€ Starting vectorized policy evaluation for {self.num_policies} policies...")
        start_time = time.perf_counter()
        
        # åˆå§‹åŒ–æœŸæœ›è‡ªç”±èƒ½æ•°ç»„
        G = np.zeros(self.num_policies)
        
        # ä¸ºæ¯ä¸ªç­–ç•¥å¹¶è¡Œè®¡ç®—æœŸæœ›è‡ªç”±èƒ½
        for policy_idx in range(self.num_policies):
            G[policy_idx] = self._compute_policy_efe_optimized(
                policy_idx, qs_current
            )
        
        # è®¡ç®—ç­–ç•¥æ¦‚ç‡åˆ†å¸ƒ (softmax)
        alpha = getattr(self.agent, 'alpha', 8.0)
        q_pi = self._softmax(-alpha * G)
        
        elapsed = time.perf_counter() - start_time
        print(f"âœ… Vectorized policy evaluation completed in {elapsed:.4f}s")
        print(f"   Average time per policy: {elapsed/self.num_policies:.6f}s")
        
        return G, q_pi
    
    def _compute_policy_efe_optimized(self, policy_idx: int, qs_current: List[np.ndarray]) -> float:
        """
        ä¼˜åŒ–çš„å•ç­–ç•¥æœŸæœ›è‡ªç”±èƒ½è®¡ç®—
        
        Args:
            policy_idx: ç­–ç•¥ç´¢å¼•
            qs_current: å½“å‰çŠ¶æ€åéªŒåˆ†å¸ƒ
            
        Returns:
            efe: è¯¥ç­–ç•¥çš„æœŸæœ›è‡ªç”±èƒ½
        """
        
        policy = self.flat_policies[policy_idx]  # (num_control_factors,)
        
        # è®¡ç®—æœŸæœ›çŠ¶æ€è½¬æ¢
        qs_next = self._compute_expected_state_transitions_optimized(policy, qs_current)
        
        # è®¡ç®—æœŸæœ›è§‚æµ‹
        qo_expected = self._compute_expected_observations_optimized(qs_next)
        
        # è®¡ç®—æœŸæœ›è‡ªç”±èƒ½ = pragmatic_value + epistemic_value
        pragmatic_value = self._compute_pragmatic_value_optimized(qo_expected)
        epistemic_value = self._compute_epistemic_value_optimized(qs_next)
        
        efe = pragmatic_value + epistemic_value
        
        return efe
    
    def _compute_expected_state_transitions_optimized(self, policy: np.ndarray, qs_current: List[np.ndarray]) -> List[np.ndarray]:
        """ä¼˜åŒ–çš„æœŸæœ›çŠ¶æ€è½¬æ¢è®¡ç®—"""
        
        qs_next = []
        
        for factor_idx in range(self.num_factors):
            qs_curr_f = qs_current[factor_idx]  # å½“å‰å› å­çš„çŠ¶æ€åˆ†å¸ƒ
            B_f = self.agent.B[factor_idx]  # è¯¥å› å­çš„è½¬æ¢çŸ©é˜µ
            
            # è·å–è¯¥ç­–ç•¥å¯¹åº”çš„åŠ¨ä½œ
            control_factors = self.agent.B_factor_control_list[factor_idx]
            
            if len(control_factors) == 1:
                # å•ä¸€æ§åˆ¶å› å­çš„æƒ…å†µ
                action = policy[control_factors[0]]
                
                if B_f.ndim == 3:  # (next_state, current_state, action)
                    B_action = B_f[:, :, action]
                else:
                    # å¤šç»´BçŸ©é˜µï¼Œéœ€è¦æ›´å¤æ‚çš„ç´¢å¼•
                    B_action = self._extract_B_slice_optimized(B_f, factor_idx, policy, qs_current)
            else:
                # å¤šä¸ªæ§åˆ¶å› å­çš„æƒ…å†µ
                B_action = self._extract_B_slice_optimized(B_f, factor_idx, policy, qs_current)
            
            # è®¡ç®—æœŸæœ›ä¸‹ä¸€çŠ¶æ€: B_action @ qs_curr_f
            if B_action.ndim == 2:
                qs_next_f = B_action @ qs_curr_f
            else:
                # å¤„ç†æ›´å¤æ‚çš„å¼ é‡ä¹˜æ³•
                qs_next_f = self._tensor_multiply_optimized(B_action, qs_curr_f, factor_idx)
            
            # ç¡®ä¿æ¦‚ç‡åˆ†å¸ƒå½’ä¸€åŒ–
            qs_next_f = qs_next_f / (qs_next_f.sum() + 1e-16)
            qs_next.append(qs_next_f)
        
        return qs_next
    
    def _extract_B_slice_optimized(self, B_factor: np.ndarray, factor_idx: int, policy: np.ndarray, qs_current: List[np.ndarray]) -> np.ndarray:
        """ä¼˜åŒ–çš„BçŸ©é˜µåˆ‡ç‰‡æå–"""
        
        # æ ¹æ®BçŸ©é˜µçš„å…·ä½“å½¢çŠ¶å’Œä¾èµ–å…³ç³»æ¥æå–ç›¸åº”çš„åˆ‡ç‰‡
        # è¿™é‡Œéœ€è¦å¤„ç†ä¸åŒå› å­çš„ä¸åŒBçŸ©é˜µç»“æ„
        
        if factor_idx == 0:  # throughput_cv: B[0] shape (6, 6, 7, 5, 7, 7)
            # ä¾èµ–å› å­: [0,1,2,3], æ§åˆ¶å› å­: [0]
            action_cv = policy[0]
            
            # ä½¿ç”¨å½“å‰çŠ¶æ€çš„è¾¹é™…åˆ†å¸ƒæ¥è®¡ç®—æœŸæœ›åˆ‡ç‰‡
            qs_1 = qs_current[1]  # quality_cv
            qs_2 = qs_current[2]  # model_size  
            qs_3 = qs_current[3]  # cores_cv
            
            # è®¡ç®—æœŸæœ›çš„Båˆ‡ç‰‡
            B_slice = np.zeros((B_factor.shape[0], B_factor.shape[1]))
            for i1 in range(len(qs_1)):
                for i2 in range(len(qs_2)):
                    for i3 in range(len(qs_3)):
                        weight = qs_1[i1] * qs_2[i2] * qs_3[i3]
                        B_slice += weight * B_factor[:, :, i1, i2, i3, action_cv]
                        
        elif factor_idx == 1:  # quality_cv: B[1] shape (7, 7, 7)
            action_cv = policy[0]
            B_slice = B_factor[:, :, action_cv]
            
        elif factor_idx == 2:  # model_size: B[2] shape (5, 5, 7)
            action_cv = policy[0]
            B_slice = B_factor[:, :, action_cv]
            
        elif factor_idx == 3:  # cores_cv: B[3] shape (7, 7, 7, 7, 5)
            action_cv = policy[0]
            action_qr = policy[1]
            qs_6 = qs_current[6]  # cores_qr
            
            # è®¡ç®—æœŸæœ›çš„Båˆ‡ç‰‡
            B_slice = np.zeros((B_factor.shape[0], B_factor.shape[1]))
            for i6 in range(len(qs_6)):
                weight = qs_6[i6]
                B_slice += weight * B_factor[:, :, i6, action_cv, action_qr]
                
        elif factor_idx == 4:  # throughput_qr: B[4] shape (6, 6, 8, 7, 5)
            action_qr = policy[1]
            qs_5 = qs_current[5]  # quality_qr
            qs_6 = qs_current[6]  # cores_qr
            
            # è®¡ç®—æœŸæœ›çš„Båˆ‡ç‰‡
            B_slice = np.zeros((B_factor.shape[0], B_factor.shape[1]))
            for i5 in range(len(qs_5)):
                for i6 in range(len(qs_6)):
                    weight = qs_5[i5] * qs_6[i6]
                    B_slice += weight * B_factor[:, :, i5, i6, action_qr]
                    
        elif factor_idx == 5:  # quality_qr: B[5] shape (8, 8, 5)
            action_qr = policy[1]
            B_slice = B_factor[:, :, action_qr]
            
        elif factor_idx == 6:  # cores_qr: B[6] shape (7, 7, 7, 7, 5)
            action_cv = policy[0]
            action_qr = policy[1]
            qs_3 = qs_current[3]  # cores_cv
            
            # è®¡ç®—æœŸæœ›çš„Båˆ‡ç‰‡
            B_slice = np.zeros((B_factor.shape[0], B_factor.shape[1]))
            for i3 in range(len(qs_3)):
                weight = qs_3[i3]
                B_slice += weight * B_factor[:, :, i3, action_cv, action_qr]
        else:
            # é»˜è®¤æƒ…å†µï¼šå‡è®¾æœ€åä¸€ä¸ªç»´åº¦æ˜¯åŠ¨ä½œ
            action = policy[0] if len(policy) > 0 else 0
            B_slice = B_factor[..., action]
            
        return B_slice
    
    def _tensor_multiply_optimized(self, B_slice: np.ndarray, qs_curr: np.ndarray, factor_idx: int) -> np.ndarray:
        """ä¼˜åŒ–çš„å¼ é‡ä¹˜æ³•"""
        
        if B_slice.ndim == 2:
            return B_slice @ qs_curr
        else:
            # å¤„ç†é«˜ç»´å¼ é‡çš„æƒ…å†µ
            # é€šå¸¸æ˜¯æ²¿ç€ç‰¹å®šè½´è¿›è¡Œsum-product
            return np.tensordot(B_slice, qs_curr, axes=([-1], [0]))
    
    def _compute_expected_observations_optimized(self, qs_next: List[np.ndarray]) -> List[np.ndarray]:
        """ä¼˜åŒ–çš„æœŸæœ›è§‚æµ‹è®¡ç®—"""
        
        qo_expected = []
        
        for factor_idx in range(self.num_factors):
            A_f = self.agent.A[factor_idx]  # è§‚æµ‹çŸ©é˜µ
            qs_next_f = qs_next[factor_idx]
            
            # è®¡ç®—æœŸæœ›è§‚æµ‹: sum over all state combinations
            # A_f çš„å½¢çŠ¶æ˜¯ [obs_dim, state_0, state_1, ..., state_6]
            
            # ä½¿ç”¨çˆ±å› æ–¯å¦æ±‚å’Œæ¥é«˜æ•ˆè®¡ç®—
            # è¿™é‡Œéœ€è¦æ ¹æ®AçŸ©é˜µçš„å…·ä½“å½¢çŠ¶æ¥è°ƒæ•´
            qo_f = self._compute_observation_likelihood_optimized(A_f, qs_next, factor_idx)
            qo_expected.append(qo_f)
        
        return qo_expected
    
    def _compute_observation_likelihood_optimized(self, A_factor: np.ndarray, qs_next: List[np.ndarray], obs_factor_idx: int) -> np.ndarray:
        """ä¼˜åŒ–çš„è§‚æµ‹ä¼¼ç„¶è®¡ç®—"""
        
        # A_factor å½¢çŠ¶: [obs_dim, state_0, state_1, ..., state_6]
        # æˆ‘ä»¬éœ€è¦è®¡ç®—: sum_{s0,s1,...,s6} A[o, s0,s1,...,s6] * P(s0) * P(s1) * ... * P(s6)
        
        # æ„å»ºçŠ¶æ€æ¦‚ç‡çš„å¤–ç§¯
        state_joint = qs_next[0]
        for i in range(1, len(qs_next)):
            state_joint = np.outer(state_joint.flatten(), qs_next[i]).flatten()
        
        # å°†çŠ¶æ€è”åˆåˆ†å¸ƒé‡å¡‘ä¸ºä¸AçŸ©é˜µåŒ¹é…çš„å½¢çŠ¶
        joint_shape = [len(qs) for qs in qs_next]
        state_joint = state_joint.reshape(joint_shape)
        
        # è®¡ç®—æœŸæœ›è§‚æµ‹
        # ä½¿ç”¨å¼ é‡ä¹˜æ³•: A[obs, :, :, ..., :] * state_joint[:, :, ..., :]
        obs_shape = A_factor.shape[0]
        qo = np.zeros(obs_shape)
        
        # ç®€åŒ–è®¡ç®—ï¼šç”±äºAæ˜¯å•ä½çŸ©é˜µç»“æ„ï¼Œç›´æ¥ä½¿ç”¨å¯¹åº”çš„çŠ¶æ€è¾¹é™…
        qo = qs_next[obs_factor_idx].copy()
        
        return qo
    
    def _compute_pragmatic_value_optimized(self, qo_expected: List[np.ndarray]) -> float:
        """ä¼˜åŒ–çš„å®ç”¨ä»·å€¼è®¡ç®—"""
        
        pragmatic_value = 0.0
        
        for factor_idx in range(self.num_factors):
            C_f = self.agent.C[factor_idx]  # åå¥½å‘é‡
            qo_f = qo_expected[factor_idx]
            
            # è®¡ç®—æœŸæœ›æ•ˆç”¨: sum_o C[o] * P(o)
            expected_utility = np.dot(C_f, qo_f)
            pragmatic_value += expected_utility
        
        return -pragmatic_value  # è´Ÿå·å› ä¸ºæˆ‘ä»¬æœ€å°åŒ–è‡ªç”±èƒ½
    
    def _compute_epistemic_value_optimized(self, qs_next: List[np.ndarray]) -> float:
        """ä¼˜åŒ–çš„è®¤çŸ¥ä»·å€¼è®¡ç®—"""
        
        # è®¤çŸ¥ä»·å€¼ = æ¡ä»¶ç†µ - ç†µ
        # è¿™é‡Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
        epistemic_value = 0.0
        
        for factor_idx in range(self.num_factors):
            qs_f = qs_next[factor_idx]
            
            # è®¡ç®—ç†µ: -sum_s P(s) * log(P(s))
            entropy = -np.sum(qs_f * np.log(qs_f + 1e-16))
            epistemic_value += entropy
        
        return -epistemic_value  # è´Ÿå·å› ä¸ºè®¤çŸ¥ä»·å€¼æ˜¯å‡å°‘ä¸ç¡®å®šæ€§
    
    def _softmax(self, x: np.ndarray) -> np.ndarray:
        """æ•°å€¼ç¨³å®šçš„softmax"""
        exp_x = np.exp(x - np.max(x))
        return exp_x / np.sum(exp_x)

def test_vectorized_policy_inference():
    """æµ‹è¯•å‘é‡åŒ–ç­–ç•¥æ¨ç†çš„æ€§èƒ½"""
    
    print("ğŸš€ VECTORIZED POLICY INFERENCE PERFORMANCE TEST")
    print("="*70)
    
    # åˆ›å»ºæµ‹è¯•æ™ºèƒ½ä½“
    print("1ï¸âƒ£  Setting up agent...")
    agent_creator = pymdp_Agent()
    agent = agent_creator.generate_agent(
        policy_length=1,
        learning_rate=1,
        alpha=8,
        action_selection="stochastic"
    )
    
    test_state = [2, 4, 1, 2, 3, 4, 1]
    
    # è¿›è¡ŒçŠ¶æ€æ¨ç†
    print("2ï¸âƒ£  Performing state inference...")
    qs_current = agent.infer_states(test_state)
    
    # æµ‹è¯•åŸå§‹ç­–ç•¥æ¨ç†
    print("3ï¸âƒ£  Testing original policy inference...")
    start_time = time.perf_counter()
    result_original = agent.infer_policies()
    original_time = time.perf_counter() - start_time
    
    if len(result_original) == 3:
        q_pi_orig, G_orig, G_sub_orig = result_original
    else:
        q_pi_orig, G_orig = result_original
    
    print(f"   Original policy inference time: {original_time:.4f}s")
    
    # æµ‹è¯•å‘é‡åŒ–ç­–ç•¥æ¨ç†
    print("4ï¸âƒ£  Testing vectorized policy inference...")
    vectorized_inference = VectorizedPolicyInference(agent)
    
    start_time = time.perf_counter()
    G_vectorized, q_pi_vectorized = vectorized_inference.vectorized_policy_evaluation(qs_current)
    vectorized_time = time.perf_counter() - start_time
    
    print(f"   Vectorized policy inference time: {vectorized_time:.4f}s")
    
    # æ€§èƒ½æ¯”è¾ƒ
    speedup = original_time / vectorized_time
    improvement = (1 - vectorized_time / original_time) * 100
    
    print(f"\nğŸ¯ PERFORMANCE COMPARISON:")
    print(f"   Original time:    {original_time:.4f}s")
    print(f"   Vectorized time:  {vectorized_time:.4f}s")
    print(f"   Speedup:          {speedup:.2f}x")
    print(f"   Improvement:      {improvement:.1f}%")
    
    # éªŒè¯ç»“æœä¸€è‡´æ€§
    print(f"\nğŸ” RESULT VERIFICATION:")
    try:
        # æ¯”è¾ƒç­–ç•¥æ¦‚ç‡åˆ†å¸ƒ
        pi_diff = np.mean(np.abs(q_pi_orig - q_pi_vectorized))
        G_diff = np.mean(np.abs(G_orig - G_vectorized))
        
        print(f"   Policy probability difference: {pi_diff:.6f}")
        print(f"   EFE difference: {G_diff:.6f}")
        
        if pi_diff < 0.01 and G_diff < 0.1:
            print(f"   âœ… Results are consistent!")
        else:
            print(f"   âš ï¸  Results differ significantly - need debugging")
            
    except Exception as e:
        print(f"   âš ï¸  Could not verify results: {e}")
    
    return {
        'original_time': original_time,
        'vectorized_time': vectorized_time,
        'speedup': speedup,
        'improvement': improvement
    }

if __name__ == "__main__":
    test_vectorized_policy_inference() 